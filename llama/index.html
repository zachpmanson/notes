<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="/static/styles/colors.css" />
    <link rel="stylesheet" href="/static/styles/global.css" />
    <link rel="stylesheet" href="/static/styles/code.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="/static/icon/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/static/icon/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="/static/icon/favicon-16x16.png"
    />
    <link rel="manifest" href="/static/icon/site.webmanifest" />
    <link rel="mask-icon" href="/static/icon/safari-pinned-tab.svg" color="#bd93f9" />
    <link rel="shortcut icon" href="/static/icon/favicon.ico" />
    <meta name="msapplication-TileColor" content="#603cba" />
    <meta name="msapplication-config" content="/static/icon/browserconfig.xml" />
    <meta name="theme-color" content="#282A36" />


    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="UTF-8" />
    <title>notes: llama</title>
  </head>

  <body>
     <nav>
      <div>
        <a href="/">
          <img src="/static/icon/icon.png" alt="notes" height="100">
        </a>
      </div>
      

      
      <div>
        
        <ul>
        
          <li>
            
            <a href="/fediverse">Fediverse</a>
            
          </li>
        
          <li>
            
            <a href="/gecos">GECOS</a>
            
          </li>
        
          <li>
            
            <a href="/generative-ai" class="activated-link">Generative AI</a>
            
          </li>
        
          <li>
            
            <a href="/mistakes">Mistakes</a>
            
          </li>
        
          <li>
            
            <a href="/programs-with-fun-names">Programs with Fun Names</a>
            
          </li>
        
          <li>
            
            <a href="/vi(m)">Vi(m)</a>
            
          </li>
        
        </ul>
        
      </div>
      

      
      <div>
        
        <ul>
        
          <li>
            
            <a href="/chatgpt">ChatGPT</a>
            
          </li>
        
          <li>
            
            <a href="/llama" class="activated-link">LLaMA</a>
            
          </li>
        
          <li>
            
            <a href="/low-rank-adaptation">Low-Rank Adaptation</a>
            
          </li>
        
          <li>
            
            <a href="/react-prompting">ReAct Prompting</a>
            
          </li>
        
          <li>
            
            <a href="/self-hosting">Self-Hosting</a>
            
          </li>
        
          <li>
            
            <a href="/stable-diffusion">Stable Diffusion</a>
            
          </li>
        
        </ul>
        
      </div>
      



      
      <div>
        
        <ul>
        
          <li>
            <a href="/alpaca">Alpaca</a>
          </li>
        
        </ul>
        
      </div>
      

    </nav>
    <main>
      <header>
        <h1>LLaMA</h1>
      </header>
      <article><p>LLaMA is "a collection of foundation language models ranging from 7B to 65B parameters" created by Meta Research. </p>
<blockquote>
<p>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>
</blockquote>
<p><cite>LLaMA paper abstract</cite></p>
<p>It was <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">announced</a> and <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">published</a> on 2023-02-24 and distributed to selected researchers not long after that.  There were several variants of the model with 7B, 13B, 33B, and 65B parameters.</p>
<blockquote>
<p>In a totally cyberpunk move, within a few days of the release, someone <a href="https://github.com/facebookresearch/llama/pull/73">submitted this PR</a> to the LLaMA repository linking to an unofficial BitTorrent download link for the model files!</p>
</blockquote>
<p><a href="https://simonwillison.net/2023/Mar/11/llama/"><cite>Simon Willison</cite></a></p>
<p>Simon Willison has interesting writeups on LLaMA and early responses to it:</p>
<ul>
<li><a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a><ul>
<li>discusses the release of LLaMA and llama.cpp</li>
</ul>
</li>
<li><a href="https://simonwillison.net/2023/Mar/13/alpaca/">Stanford Alpaca, and the acceleration of on-device large language model development</a><ul>
<li>discusses Dalai and Stanford Alpaca</li>
</ul>
</li>
<li><a href="https://simonwillison.net/2023/Mar/17/beat-chatgpt-in-a-browser/">Could you train a ChatGPT-beating model for $85,000 and run it in a browser?</a><ul>
<li>discusses alpaca.cpp, <a href="/react-prompting">ReAct Prompting</a> and the ability to easily expand the capabilities of LLMs, even basic ones</li>
</ul>
</li>
<li><a href="https://simonwillison.net/tags/llama/">llama tag</a> on Willison's blog</li>
</ul>
<h2 id="consequential-projects">Consequential Projects</h2>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> by Georgi Gerganov, a port of Facebook's LLaMA model in C/C++<ul>
<li><a href="https://github.com/NouamaneTazi/bloomz.cpp">bloomz.cpp</a>, a port of BLOOM built on top of llama.cpp</li>
</ul>
</li>
<li><a href="https://github.com/cocktailpeanut/dalai">Dalai</a> by @cocktailpeanut, a single command installer to run LLaMa locally with a web interface and API<ul>
<li>later expanded to include alpaca.cpp</li>
</ul>
</li>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford Alpaca</a>, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations to function like <a href="/chatgpt">ChatGPT</a><ul>
<li><a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a>, a recreation of the Stanford Alpaca model, based on LLaMa 7B and the Stanford instruction training set using <a href="/low-rank-adaptation">Low-Rank Adaptation</a> instruct tuning<ul>
<li><a href="https://github.com/antimatter15/alpaca.cpp">alpaca.cpp</a>, a fork of llama.cpp that uses alpaca-lora</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="magnets">Magnets</h2>
<ul>
<li><a href="magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;dn=LLaMA">Original LLaMA model weights</a></li>
<li>alpaca.cpp <ul>
<li><a href="magnet:?xt=urn:btih:5aaceaec63b03e51a98f04fd5c42320b2a033010&amp;dn=ggml-alpaca-7b-q4.bin&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Fopentracker.i2p.rocks%3A6969%2Fannounce">7B model weights ggml-alpaca-7b-q4.bin</a>, 4.21GB</li>
<li><a href="magnet:?xt=urn:btih:f3cf71b172129d6b5abccab393bc32253fac8159&amp;dn=ggml-alpaca-13b-q4.bin&amp;tr=udp%3A%2F%http://2Ftracker.opentrackr.org%3A1337%2Fannounce&amp;tr=udp%3A%2F%https://t.co/zenhelfwRd%3A6969%2Fannounce&amp;tr=https%3A%2F%https://t.co/zenhelfwRd%3A443%2Fannounce&amp;tr=udp%3A%2F%https://t.co/RRAn1X65wE%3A6969%2Fannounce&amp;tr=udp%3A%2F%https://t.co/uTXBeTLUMa%3A2810%2Fannounce">13B model weights</a></li>
</ul>
</li>
</ul></article>

      
      <p class="backlinks"><span class="bold">Incoming:</span>
          
          <a href="/recent-changes">Recent Changes</a>
          
          <a href="/alpaca">Alpaca</a>
          
        </span>
      </p>
      
        


      
        
        <h2><a href="/alpaca">Alpaca</a></h2>
        
      

    </main>
  </body>
</html>