<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="/static/styles/colors.css" />
    <link rel="stylesheet" href="/static/styles/global.css" />
    <link rel="stylesheet" href="/static/styles/code.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="/static/icon/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/static/icon/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="/static/icon/favicon-16x16.png"
    />
    <link rel="manifest" href="/static/icon/site.webmanifest" />
    <link rel="mask-icon" href="/static/icon/safari-pinned-tab.svg" color="#bd93f9" />
    <link rel="shortcut icon" href="/static/icon/favicon.ico" />
    <meta name="msapplication-TileColor" content="#603cba" />
    <meta name="msapplication-config" content="/static/icon/browserconfig.xml" />
    <meta name="theme-color" content="#282A36" />


    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="UTF-8" />
    <title>notes: llama</title>
  </head>

  <body>
     <nav>
      <div>
        <a href="/">
          <img src="/static/icon/icon.png" alt="notes" height="100">
        </a>
      </div>
      
      
      <div>
        <ul>
        
          <li>
            
            <a href="/machine-learning" class="activated-link">Machine Learning</a>
            
          </li>
        
          <li>
            
            <a href="/mistakes">Mistakes</a>
            
          </li>
        
          <li>
            
            <a href="/software">Software</a>
            
          </li>
        
        </ul>
      </div>
      

      
      <div>
        <ul>
        
          <li>
            
            <a href="/chatgpt">ChatGPT</a>
            
          </li>
        
          <li>
            
            <a href="/llama" class="activated-link">LLaMa</a>
            
          </li>
        
          <li>
            
            <a href="/stable-diffusion">Stable Diffusion</a>
            
          </li>
        
        </ul>
      </div>
      

      

    </nav>
    <main>
      <header>
        <h1>LLaMa</h1>
      </header>
      <article><p>LLaMa is "a collection of foundation language models ranging from 7B to 65B parameters" created by Meta Research.</p>
<blockquote>
<p>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>
</blockquote>
<p><cite>LLaMa paper abstract</cite></p>
<p>It was <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">announced</a> and <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">published</a> on 2023-02-24 and distributed to selected researchers not long after that.  There were several variants of the model with 7B, 13B, 33B, and 65B parameters.</p>
<blockquote>
<p>In a totally cyberpunk move, within a few days of the release, someone <a href="https://github.com/facebookresearch/llama/pull/73">submitted this PR</a> to the LLaMA repository linking to an unofficial BitTorrent download link for the model files!</p>
</blockquote>
<p><a href="https://simonwillison.net/2023/Mar/11/llama/"><cite>Simon Willison</cite></a></p>
<p>Simon Willison has interesting writeups on LLaMa and early responses to it:</p>
<ul>
<li><a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a></li>
<li><a href="https://simonwillison.net/2023/Mar/13/alpaca/">Stanford Alpaca, and the acceleration of on-device large language model development</a></li>
<li><a href="https://simonwillison.net/tags/llama/">llama tag</a> on Willison's blog</li>
</ul>
<h2 id="consequential-projects">Consequential Projects</h2>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> by Georgi Gerganov, a port of Facebook's LLaMA model in C/C++</li>
<li><a href="https://github.com/cocktailpeanut/dalai">Dalai</a> by @cocktailpeanut, a single command installer to run LLaMa locally with a web interface and API</li>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford Alpaca</a>, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations to function like ChatGPT</li>
</ul></article>
      
      

      
        
      


    </main>
  </body>
</html>