<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="/static/styles/colors.css" />
    <link rel="stylesheet" href="/static/styles/global.css" />
    <link rel="stylesheet" href="/static/styles/code.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="/static/icon/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/static/icon/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="/static/icon/favicon-16x16.png"
    />
    <link rel="manifest" href="/static/icon/site.webmanifest" />
    <link rel="mask-icon" href="/static/icon/safari-pinned-tab.svg" color="#bd93f9" />
    <link rel="shortcut icon" href="/static/icon/favicon.ico" />
    <meta name="msapplication-TileColor" content="#603cba" />
    <meta name="msapplication-config" content="/static/icon/browserconfig.xml" />
    <meta name="theme-color" content="#282A36" />


    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="UTF-8" />
    <title>notes: llama</title>
  </head>

  <body>
     <nav>
      <div>
        <a href="/">
          <img src="/static/icon/icon.png" alt="notes" height="100">
        </a>
      </div>
      

      
      <div>
        
        <ul>
        
          <li>
            
            <a href="/ai-concepts">AI Concepts</a>
            
          </li>
        
          <li>
            
            <a href="/large-language-models" class="activated-link">Large Language Models</a>
            
          </li>
        
          <li>
            
            <a href="/practical-deep-learning">Practical Deep Learning</a>
            
          </li>
        
          <li>
            
            <a href="/stable-diffusion">Stable Diffusion</a>
            
          </li>
        
        </ul>
        
      </div>
      

      
      <div>
        
        <ul>
        
          <li>
            
            <a href="/chatgpt">ChatGPT</a>
            
          </li>
        
          <li>
            
            <a href="/llama" class="activated-link">LLaMA</a>
            
          </li>
        
        </ul>
        
      </div>
      



      
      <div>
        
        <ul>
        
          <li>
            <a href="/alpaca">Alpaca</a>
          </li>
        
          <li>
            <a href="/llama-family-tree">LLaMA Family Tree</a>
          </li>
        
        </ul>
        
      </div>
      

    </nav>
    <main>
      <header>
        <h1>LLaMA</h1>
      </header>
      <article><p>LLaMA is "a collection of foundation language models ranging from 7B to 65B parameters" created by Meta Research. </p>
<blockquote>
<p>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>
</blockquote>
<p><cite> <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">LLaMA: Open and Efficient Foundation Language Models (Abstract)</a></cite></p>
<p>It was <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">announced</a> and <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">published</a> on 2023-02-24 and distributed to selected researchers not long after that.  There were several variants of the model with 7B, 13B, 33B, and 65B parameters.</p>
<blockquote>
<p>In a totally cyberpunk move, within a few days of the release, someone <a href="https://github.com/facebookresearch/llama/pull/73">submitted this PR</a> to the LLaMA repository linking to an unofficial BitTorrent download link for the model files!</p>
</blockquote>
<p><cite> <a href="https://simonwillison.net/2023/Mar/11/llama/">Simon Willison</a></cite></p>
<p>Simon Willison has interesting writeups on LLaMA and early responses to it:</p>
<ul>
<li><a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a><ul>
<li>discusses the release of LLaMA and llama.cpp</li>
</ul>
</li>
<li><a href="https://simonwillison.net/2023/Mar/13/alpaca/">Stanford Alpaca, and the acceleration of on-device large language model development</a><ul>
<li>discusses Dalai and Stanford <a href="/alpaca">Alpaca</a></li>
</ul>
</li>
<li><a href="https://simonwillison.net/2023/Mar/17/beat-chatgpt-in-a-browser/">Could you train a ChatGPT-beating model for $85,000 and run it in a browser?</a><ul>
<li>discusses alpaca.cpp, <a href="/react-prompting">ReAct Prompting</a> and the ability to easily expand the capabilities of LLMs, even basic ones</li>
</ul>
</li>
<li><a href="https://simonwillison.net/tags/llama/">llama tag</a> on Willison's blog</li>
</ul>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, a port of Facebook's LLaMA model in C/C++<ul>
<li>designed to run the model using 4-bit <a href="/quantization">quantization</a> on a MacBook</li>
</ul>
</li>
</ul>
<h2 id="magnets">Magnets</h2>
<ul>
<li><a href="magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;dn=LLaMA">Original LLaMA model weights</a></li>
</ul></article>

      
      <p class="backlinks"><span class="bold">Incoming:</span>
          
          <a href="/alpaca">Alpaca</a>
          
          <a href="/llama-family-tree">LLaMA Family Tree</a>
          
        </span>
      </p>
      
        


      
        
        <h2><a href="/alpaca">Alpaca</a></h2>
        
        <h2><a href="/llama-family-tree">LLaMA Family Tree</a></h2>
        
      

    </main>
  </body>
</html>