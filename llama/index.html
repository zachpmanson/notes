<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/static/styles/global.css" />
  <link rel="stylesheet" href="/static/styles/code.css" />

  <link rel="icon" href="/static/icon/icon.svg" type="image/svg+xml">
  <link rel="shortcut icon" href="/static/icon/favicon.ico" />

  <link rel="alternate" type="application/rss+xml" href="https://notes.zachmanson.com/posts.xml" title="notes: posts" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta charset="UTF-8" />
  <title>notes: llama</title>
</head>

<body>
  <nav>
    <div class="main-icon">
      <a href="/">
        <svg xmlns="http://www.w3.org/2000/svg" baseProfile="full" version="1.1" style="fill: none;"
          viewBox="26.88 56.87 216.25 156.27">
          <path
            style="stroke-width: 15px; stroke-linecap: butt; stroke-linejoin: round; stroke: rgb(10, 10, 10); fill: none;"
            d="M135,75 A60,60 0 0,0 75,135 A60,60 0 0,0 135,195 A60,60 0 0,0 195,135 A60,60 0 0,0 135,75 M45,90 A15,15 0 0,1 60,75 A15,15 0 0,1 75,90 L75,135 L75,180 A15,15 0 0,1 60,195 A15,15 0 0,1 45,180 M225,90 A15,15 0 0,0 210,75 A15,15 0 0,0 195,90 L195,135 L195,180 A15,15 0 0,0 210,195 L210,195 A15,15 0 0,0 225,180 M135,105 A30,30 0 0,1 165,135 A30,30 0 0,1 135,165 A30,30 0 0,1 105,135 A30,30 0 0,1 135,105 M45,105 L45,105 L45,165 M225,105 L225,105 L225,165 ">
          </path>
        </svg>
      </a>
    </div>

    <div class="nav-segments-container">
      <div class="nav-segments">
        

        <div class="flex flex-col text-left nav-links">
          <a href="/large-language-models" class="activated-link">Large Language Models</a>
          
          
          
          
          
          <a href="/stable-diffusion">Stable Diffusion</a>
          
          
        </div>
        

        

        <div class="flex flex-col text-left nav-links">
          <a href="/llama" class="activated-link">LLaMA</a>
          
          
          <a href="/chatgpt">ChatGPT</a>
          
          
          
          
          
          
          <a href="/tricking-llms">Tricking LLMs</a>
          
          
        </div>
        

        

        <div class="flex flex-col text-left nav-links">
          
          <a href="/alpaca">Alpaca</a>
          
          <a href="/llama-family-tree">LLaMA Family Tree</a>
          
          <a href="/llama-timeline">LLaMA Timeline</a>
          
        </div>
        
      </div>
    </div>
  </nav>

  <main>
    <header>
      <h1>LLaMA</h1>
    </header>

    <article><p>LLaMA is "a collection of foundation language models ranging from 7B to 65B parameters" created by Meta Research. </p>
<blockquote>
<p>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>
</blockquote>
<p><cite class="standalone"><a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">LLaMA: Open and Efficient Foundation Language Models (Abstract)</a></cite></p>
<p>It was <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">announced</a> and <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">published</a> on 2023-02-24 and distributed to selected researchers not long after that.  There were several variants of the model with 7B, 13B, 33B, and 65B parameters.</p>
<blockquote>
<p>In a totally cyberpunk move, within a few days of the release, someone <a href="https://github.com/facebookresearch/llama/pull/73">submitted this PR</a> to the LLaMA repository linking to an unofficial BitTorrent download link for the model files!</p>
</blockquote>
<p><cite class="standalone"><a href="https://simonwillison.net/2023/Mar/11/llama/">Simon Willison</a></cite></p>
<p>Simon Willison has interesting writeups on LLaMA and early responses to it:</p>
<ul>
<li><a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a><ul>
<li>discusses the release of LLaMA and llama.cpp</li>
</ul>
</li>
<li><a href="https://simonwillison.net/2023/Mar/13/alpaca/">Stanford Alpaca, and the acceleration of on-device large language model development</a><ul>
<li>discusses Dalai and Stanford <a href="/alpaca">Alpaca</a></li>
</ul>
</li>
<li><a href="https://simonwillison.net/2023/Mar/17/beat-chatgpt-in-a-browser/">Could you train a ChatGPT-beating model for $85,000 and run it in a browser?</a><ul>
<li>discusses alpaca.cpp, <a href="/react-prompting">ReAct Prompting</a> and the ability to easily expand the capabilities of LLMs, even basic ones</li>
</ul>
</li>
<li><a href="https://simonwillison.net/tags/llama/">llama tag</a> on Willison's blog</li>
</ul>
<h2 id="impact">Impact</h2>
<blockquote>
<p>Paradoxically, the one clear winner in all of this is Meta. Because the leaked model was theirs, they have effectively garnered an entire planet's worth of free labor. Since most open source innovation is happening on top of their architecture, there is nothing stopping them from directly incorporating it into their products.</p>
</blockquote>
<p><cite class="standalone"><a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">Leaked Internal Google Document</a> on open source LLM progress</cite></p>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, a port of Facebook's LLaMA model in C/C++<ul>
<li>designed to run the model using 4-bit <a href="/quantization">Quantization</a> on a MacBook</li>
</ul>
</li>
</ul>
<h2 id="magnets">Magnets</h2>
<ul>
<li><a href="magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;dn=LLaMA">Original LLaMA model weights</a></li>
</ul></article>
  </main>

  <footer>
    <details>
      <summary>
        <div class="footer-row">
          
          meta
          
          <a href="/recent-changes">recent</a>
          <a href="/macos-problems" class="not-extra-small">random</a>
          <a href="/posts">posts</a>
          <a href="/sitemap" class="not-extra-small">sitemap</a>
          <a href="https://github.dev/zachpmanson/notes/blob/main/./notes/Notes/Software/Machine Learning/Generative Models/Large Language Models/LLaMA/LLaMA.md" class="desktop">edit</a>
          <a href="https://github.com/zachpmanson/notes/edit/main/./notes/Notes/Software/Machine Learning/Generative Models/Large Language Models/LLaMA/LLaMA.md" class="mobile">edit</a>
          <a href="mailto:zachpmanson@gmail.com?subject=On 'LLaMA'">reply</a>
        </div>
      </summary>
      <div class="extra-small">
        <div class="footer-row">
          <a href="/sitemap">sitemap</a>
          <a href="/macos-problems">random</a>
        </div>
      </div>

      

      <p class="backlinks"><strong>Incoming:</strong>
        
        <a href="/alpaca">Alpaca</a>
        
        <a href="/llama-family-tree">LLaMA Family Tree</a>
        
        <a href="/pirate-libraries">Pirate Libraries</a>
        
        <a href="/program-names">Program Names</a>
        
        </span>
      </p>
      

      
      <p>
        

        <span class="backlinks">
          <strong>
            Updated:
          </strong>
          <date>2024-04-04</date>
        </span>

      </p>
    </details>

  </footer>
  
</body>

</html>