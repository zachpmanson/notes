Notes on various LLMs and the techniques used to make them.

>LLMs are better thought of as "calculators for words" - retrieval of facts is a by-product of how they are trained, but it's not their core competence at all.

[<cite>Simon Willison</cite>](https://news.ycombinator.com/item?id=35396372) on HN, which he later [expanded on](https://simonwillison.net/2023/Apr/2/calculator-for-words/)

## LLM Families

![](https://raw.githubusercontent.com/pavo-etc/llm-family-tree/master/LLMfamily.drawio.png)  
[Source](https://github.com/pavo-etc/llm-family-tree)

## Comparisons 

>>hyperopt:  
>>Does anyone know of any good test suites we can use to benchmark these local models? \[...\]
>
>aiappreciator:  
>The simplest and quickest benchmark is to do a rap battle between GPT-4 and the local models. \[...\]
>
>It is instantly clear how strong the model is relative to GPT-4.

[<cite>this HN thread</cite>](https://news.ycombinator.com/item?id=35349853)

## Links

- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM), a curated list of papers about large language models, especially relating to ChatGPT. It also contains frameworks for LLM training, tools to deploy LLM, courses and tutorials about LLM and all publicly available LLM checkpoints and APIs.
- [LLM Tracker](https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4), a spreadsheet detailing the different LLMs and their properties