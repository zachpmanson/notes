<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/static/styles/global.css" />
  <link rel="stylesheet" href="/static/styles/code.css" />

  <link rel="icon" href="/static/icon/icon.svg" type="image/svg+xml">
  <link rel="shortcut icon" href="/static/icon/favicon.ico" />

  <link rel="alternate" type="application/rss+xml" href="https://notes.zachmanson.com/posts.xml" title="notes: posts" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta charset="UTF-8" />
  <title>notes: llama family tree</title>
</head>

<body>
  <nav>
    <div class="main-icon">
      <a href="/">
        <svg xmlns="http://www.w3.org/2000/svg" baseProfile="full" version="1.1" style="fill: none;"
          viewBox="26.88 56.87 216.25 156.27">
          <path
            style="stroke-width: 15px; stroke-linecap: butt; stroke-linejoin: round; stroke: rgb(10, 10, 10); fill: none;"
            d="M135,75 A60,60 0 0,0 75,135 A60,60 0 0,0 135,195 A60,60 0 0,0 195,135 A60,60 0 0,0 135,75 M45,90 A15,15 0 0,1 60,75 A15,15 0 0,1 75,90 L75,135 L75,180 A15,15 0 0,1 60,195 A15,15 0 0,1 45,180 M225,90 A15,15 0 0,0 210,75 A15,15 0 0,0 195,90 L195,135 L195,180 A15,15 0 0,0 210,195 L210,195 A15,15 0 0,0 225,180 M135,105 A30,30 0 0,1 165,135 A30,30 0 0,1 135,165 A30,30 0 0,1 105,135 A30,30 0 0,1 135,105 M45,105 L45,105 L45,165 M225,105 L225,105 L225,165 ">
          </path>
        </svg>
      </a>
    </div>

    <div class="nav-segments-container">
      <div class="nav-segments">
        

        <div class="flex flex-col text-left nav-links">
          <a href="/llama" class="activated-link">LLaMA</a>
          
          
          <a href="/chatgpt">ChatGPT</a>
          
          
          
          
          
          
          <a href="/tricking-llms">Tricking LLMs</a>
          
          
        </div>
        

        

        <div class="flex flex-col text-left nav-links">
          <a href="/llama-family-tree" class="activated-link">LLaMA Family Tree</a>
          
          
          <a href="/alpaca">Alpaca</a>
          
          
          
          
          
          
          <a href="/llama-timeline">LLaMA Timeline</a>
          
          
        </div>
        

        
      </div>
    </div>


  </nav>
  <main>
    <header>
      <h1>LLaMA Family Tree</h1>
    </header>
    <article><p>LLaMA's consequential projects.</p>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> by Georgi Gerganov, a port of Facebook's <a href="/llama">LLaMA</a> model in C/C++<ul>
<li><a href="https://github.com/NouamaneTazi/bloomz.cpp">bloomz.cpp</a>, a port of BLOOM built on top of llama.cpp</li>
<li><a href="https://github.com/nomic-ai/gpt4all">gpt4all</a>, an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa</li>
</ul>
</li>
<li><a href="https://github.com/cocktailpeanut/dalai">Dalai</a> by @cocktailpeanut, a single command installer to run LLaMa locally with a web interface and API<ul>
<li>later expanded to include alpaca.cpp</li>
</ul>
</li>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford Alpaca</a>, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations to function like <a href="/chatgpt">ChatGPT</a><ul>
<li><a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a>, a recreation of Stanford <a href="/alpaca">Alpaca</a>, based on LLaMA 7B and the Stanford instruction training set using <a href="/low-rank-adaptation">Low-Rank Adaptation</a> instruct tuning<ul>
<li><a href="https://github.com/antimatter15/alpaca.cpp">alpaca.cpp</a>, a fork of llama.cpp that uses alpaca-lora</li>
<li>many children of this who have followed the method for the larger LLaMA models, such as <a href="https://huggingface.co/chansung/alpaca-lora-13b">chansung/alpaca-lora-13B</a> and <a href="https://huggingface.co/chansung/alpaca-lora-30b">chansung/alpaca-lora-30B</a></li>
</ul>
</li>
<li><a href="https://huggingface.co/chavinlo/alpaca-native">alpaca-native</a>, a recreation of Standford Alpaca, based on LLaMA 7B and the Stanford instruction training set using native fine-tuning (not LoRA) <ul>
<li><a href="https://huggingface.co/ozcur/alpaca-native-4bit">alpaca-native-4bit</a>, a 4-bit quantisation of alpaca-native made with <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMA</a> (which uses <a href="https://arxiv.org/abs/2210.17323">GPTQ</a>)</li>
</ul>
</li>
<li><a href="https://github.com/databrickslabs/dolly">Databricks Dolly</a>, a model fine-tuned from GPT-J using the Alpaca training set, demonstrating surprisingly high quality instruction following behavior not characteristic of GPT-J</li>
</ul>
</li>
<li><a href="https://vicuna.lmsys.org/">Vicuna-13B</a>, a model fine-tuned from LLaMA-13B based on 70k conversations from <a href="https://sharegpt.com/">ShareGPT</a>, with a training method improving upon Alpaca's</li>
</ul>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://agi-sphere.com/llama-models/">AGI Sphere's writeup on the LLaMa family</a>, as of April 2023</li>
</ul></article>

    <hr />

    <div class="meta">
      
      <p class="backlinks"><span class="bold">Incoming:</span>
        
        <a href="/alpaca">Alpaca</a>
        
        </span>
      </p>
      

      

      
    </div>
  </main>
  <footer>
    <a href="mailto:zachpmanson@gmail.com?subject=On 'LLaMA Family Tree'">reply</a>
    <a href="/alculator">random</a>
    <a href="/recent-changes">updates</a>
    <a href="/posts">posts</a>
    <a href="/tags">tags</a>
    <a href="/sitemap">sitemap</a>
    <a href="https://github.dev/pavo-etc/notes/blob/main/./notes/Notes/Software/Machine Learning/Generative Models/Large Language Models/LLaMA/LLaMA Family Tree.md" class="desktop">edit</a>
    <a href="https://github.com/pavo-etc/notes/edit/main/./notes/Notes/Software/Machine Learning/Generative Models/Large Language Models/LLaMA/LLaMA Family Tree.md" class="mobile">edit</a>
  </footer>
  
</body>

</html>