<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/static/styles/colors.css" />
  <link rel="stylesheet" href="/static/styles/global.css" />
  <link rel="stylesheet" href="/static/styles/code.css" />

  <link rel="apple-touch-icon" sizes="180x180" href="/static/icon/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/static/icon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/static/icon/favicon-16x16.png" />
  <link rel="manifest" href="/static/icon/site.webmanifest" />
  <link rel="mask-icon" href="/static/icon/safari-pinned-tab.svg" color="#bd93f9" />
  <link rel="shortcut icon" href="/static/icon/favicon.ico" />
  <meta name="msapplication-TileColor" content="#603cba" />
  <meta name="msapplication-config" content="/static/icon/browserconfig.xml" />
  <meta name="theme-color" content="#282A36" />


  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta charset="UTF-8" />
  <title>notes: llama family tree</title>
</head>

<body>
  <nav>

    <div class="main-icon">
      <a href="/">
        <img src="/static/icon/icon.png" alt="notes" height="100">
      </a>
    </div>

    <div class="nav-segments-container">
      <div class="nav-segments">
        

        <div class="flex flex-col text-left nav-links">
          
          
          <a href="/chatgpt">ChatGPT</a>
          
          
          
          <a href="/llama" class="activated-link">LLaMA</a>
          
          
        </div>
        

        

        <div class="flex flex-col text-left nav-links">
          
          
          <a href="/alpaca">Alpaca</a>
          
          
          
          <a href="/llama-family-tree" class="activated-link">LLaMA Family Tree</a>
          
          
          
          <a href="/llama-timeline">LLaMA Timeline</a>
          
          
        </div>
        

        
      </div>
    </div>

  </nav>
  <main>
    <header>
      <h1>LLaMA Family Tree</h1>
    </header>
    <article><p>LLaMA's consequential projects.</p>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> by Georgi Gerganov, a port of Facebook's <a href="/llama">LLaMA</a> model in C/C++<ul>
<li><a href="https://github.com/NouamaneTazi/bloomz.cpp">bloomz.cpp</a>, a port of BLOOM built on top of llama.cpp</li>
<li><a href="https://github.com/nomic-ai/gpt4all">gpt4all</a>, an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa</li>
</ul>
</li>
<li><a href="https://github.com/cocktailpeanut/dalai">Dalai</a> by @cocktailpeanut, a single command installer to run LLaMa locally with a web interface and API<ul>
<li>later expanded to include alpaca.cpp</li>
</ul>
</li>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford Alpaca</a>, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations to function like <a href="/chatgpt">ChatGPT</a><ul>
<li><a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a>, a recreation of Stanford <a href="/alpaca">Alpaca</a>, based on LLaMA 7B and the Stanford instruction training set using <a href="/low-rank-adaptation">Low-Rank Adaptation</a> instruct tuning<ul>
<li><a href="https://github.com/antimatter15/alpaca.cpp">alpaca.cpp</a>, a fork of llama.cpp that uses alpaca-lora</li>
<li>many children of this who have followed the method for the larger LLaMA models, such as <a href="https://huggingface.co/chansung/alpaca-lora-13b">chansung/alpaca-lora-13B</a> and <a href="https://huggingface.co/chansung/alpaca-lora-30b">chansung/alpaca-lora-30B</a></li>
</ul>
</li>
<li><a href="https://huggingface.co/chavinlo/alpaca-native">alpaca-native</a>, a recreation of Standford Alpaca, based on LLaMA 7B and the Stanford instruction training set using native fine-tuning (not LoRA) <ul>
<li><a href="https://huggingface.co/ozcur/alpaca-native-4bit">alpaca-native-4bit</a>, a 4-bit quantisation of alpaca-native made with <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMA</a> (which uses <a href="https://arxiv.org/abs/2210.17323">GPTQ</a>)</li>
</ul>
</li>
<li><a href="https://github.com/databrickslabs/dolly">Databricks Dolly</a>, a model fine-tuned from GPT-J using the Alpaca training set, demonstrating surprisingly high quality instruction following behavior not characteristic of GPT-J</li>
</ul>
</li>
<li><a href="https://vicuna.lmsys.org/">Vicuna-13B</a>, a model fine-tuned from LLaMA-13B based on 70k conversations from <a href="https://sharegpt.com/">ShareGPT</a>, with a training method improving upon Alpaca's</li>
</ul>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://agi-sphere.com/llama-models/">AGI Sphere's writeup on the LLaMa family</a>, as of April 2023</li>
</ul></article>

    
    <p class="backlinks"><span class="bold">Incoming:</span>
      
      <a href="/alpaca">Alpaca</a>
      
      </span>
    </p>
    

    




    
    
    

  </main>
  <footer>
    <a href="https://github.dev/pavo-etc/notes/blob/main/./notes/Notes/Software/Machine Learning/Generative Models/Large Language Models/LLaMA/LLaMA Family Tree.md">edit</a> -
    <a href="/best-practices-for-time-travellers">random</a> -
    <a href="/recent-changes">updates</a> -
    <a href="/tags">tags</a> -
    <a href="/sitemap">sitemap</a>
  </footer>
  
</body>

</html>