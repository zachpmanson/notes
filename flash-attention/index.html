<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="/static/styles/colors.css" />
    <link rel="stylesheet" href="/static/styles/global.css" />
    <link rel="stylesheet" href="/static/styles/code.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="/static/icon/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/static/icon/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="/static/icon/favicon-16x16.png"
    />
    <link rel="manifest" href="/static/icon/site.webmanifest" />
    <link rel="mask-icon" href="/static/icon/safari-pinned-tab.svg" color="#bd93f9" />
    <link rel="shortcut icon" href="/static/icon/favicon.ico" />
    <meta name="msapplication-TileColor" content="#603cba" />
    <meta name="msapplication-config" content="/static/icon/browserconfig.xml" />
    <meta name="theme-color" content="#282A36" />


    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="UTF-8" />
    <title>notes: flash attention</title>
  </head>

  <body>
     <nav>
      <div>
        <a href="/">
          <img src="/static/icon/icon.png" alt="notes" height="100">
        </a>
      </div>
      

      
      <div>
        
        <ul>
        
          <li>
            
            <a href="/ai-concepts" class="activated-link">AI Concepts</a>
            
          </li>
        
          <li>
            
            <a href="/large-language-models">Large Language Models</a>
            
          </li>
        
          <li>
            
            <a href="/practical-deep-learning">Practical Deep Learning</a>
            
          </li>
        
          <li>
            
            <a href="/stable-diffusion">Stable Diffusion</a>
            
          </li>
        
        </ul>
        
      </div>
      

      
      <div>
        
        <ul>
        
          <li>
            
            <a href="/few-shot-learning">Few-Shot Learning</a>
            
          </li>
        
          <li>
            
            <a href="/flash-attention" class="activated-link">Flash Attention</a>
            
          </li>
        
          <li>
            
            <a href="/low-rank-adaptation">Low-Rank Adaptation</a>
            
          </li>
        
          <li>
            
            <a href="/quantization">Quantization</a>
            
          </li>
        
          <li>
            
            <a href="/react-prompting">ReAct Prompting</a>
            
          </li>
        
        </ul>
        
      </div>
      



      

    </nav>
    <main>
      <header>
        <h1>Flash Attention</h1>
      </header>
      <article><p>Flash attention is "an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM", allowing longer context windows in Transformers.</p>
<p>Read more about it <a href="https://arxiv.org/abs/2205.14135">here</a>, I still don't get it.</p></article>

      
      <p class="backlinks"><span class="bold">Incoming:</span>
          
          <a href="/recent-changes">Recent Changes</a>
          
        </span>
      </p>
      

      

        


      
        
      

    </main>
  </body>
</html>