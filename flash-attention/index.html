<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/static/styles/colors.css" />
  <link rel="stylesheet" href="/static/styles/global.css" />
  <link rel="stylesheet" href="/static/styles/code.css" />

  <link rel="apple-touch-icon" sizes="180x180" href="/static/icon/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/static/icon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/static/icon/favicon-16x16.png" />
  <link rel="manifest" href="/static/icon/site.webmanifest" />
  <link rel="mask-icon" href="/static/icon/safari-pinned-tab.svg" color="#bd93f9" />
  <link rel="shortcut icon" href="/static/icon/favicon.ico" />
  <meta name="msapplication-TileColor" content="#603cba" />
  <meta name="msapplication-config" content="/static/icon/browserconfig.xml" />
  <meta name="theme-color" content="#282A36" />


  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta charset="UTF-8" />
  <title>notes: flash attention</title>
</head>

<body>
  <nav>

    <div class="main-icon">
      <a href="/">
        <img src="/static/icon/icon.png" alt="notes" height="100">
      </a>
    </div>

    <div class="nav-segments-container">
      <div class="nav-segments">
        

        <div class="flex flex-col text-left nav-links">
          
          
          <a href="/generative-models">Generative Models</a>
          
          
          
          <a href="/limitations-of-machine-learning">Limitations of Machine Learning</a>
          
          
          
          <a href="/machine-learning-concepts" class="activated-link">Machine Learning Concepts</a>
          
          
          
          <a href="/neural-networks">Neural Networks</a>
          
          
          
          <a href="/pytorch">PyTorch</a>
          
          
        </div>
        

        

        <div class="flex flex-col text-left nav-links">
          
          
          <a href="/few-shot-learning">Few-Shot Learning</a>
          
          
          
          <a href="/flash-attention" class="activated-link">Flash Attention</a>
          
          
          
          <a href="/low-rank-adaptation">Low-Rank Adaptation</a>
          
          
          
          <a href="/quantization">Quantization</a>
          
          
          
          <a href="/react-prompting">ReAct Prompting</a>
          
          
        </div>
        

        
      </div>
    </div>

  </nav>
  <main>
    <header>
      <h1>Flash Attention</h1>
    </header>
    <article><p>Flash attention is "an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM", allowing longer context windows in Transformers.</p>
<p>Read more about it <a href="https://arxiv.org/abs/2205.14135">here</a>, I still don't get it.</p></article>

    

    




    
    
    

  </main>
  <footer>
    <a href="https://github.dev/pavo-etc/notes/blob/main/./notes/Notes/Software/Machine Learning/Machine Learning Concepts/Flash Attention.md">edit</a> -
    <a href="/development">random</a> -
    <a href="/recent-changes">updates</a> -
    <a href="/tags">tags</a> -
    <a href="/sitemap">sitemap</a>
  </footer>
  
</body>

</html>