<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/static/styles/global.css" />
  <link rel="stylesheet" href="/static/styles/code.css" />

  <link rel="icon" href="/static/icon/icon.svg" type="image/svg+xml">
  <link rel="shortcut icon" href="/static/icon/favicon.ico" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta charset="UTF-8" />
  <title>notes: alpaca</title>
</head>

<body>
  <nav>
    <div class="main-icon">
      <a href="/">
        <svg xmlns="http://www.w3.org/2000/svg" baseProfile="full" version="1.1" style="fill: none;"
          viewBox="26.88 56.87 216.25 156.27">
          <path
            style="stroke-width: 15px; stroke-linecap: butt; stroke-linejoin: round; stroke: rgb(10, 10, 10); fill: none;"
            d="M135,75 A60,60 0 0,0 75,135 A60,60 0 0,0 135,195 A60,60 0 0,0 195,135 A60,60 0 0,0 135,75 M45,90 A15,15 0 0,1 60,75 A15,15 0 0,1 75,90 L75,135 L75,180 A15,15 0 0,1 60,195 A15,15 0 0,1 45,180 M225,90 A15,15 0 0,0 210,75 A15,15 0 0,0 195,90 L195,135 L195,180 A15,15 0 0,0 210,195 L210,195 A15,15 0 0,0 225,180 M135,105 A30,30 0 0,1 165,135 A30,30 0 0,1 135,165 A30,30 0 0,1 105,135 A30,30 0 0,1 135,105 M45,105 L45,105 L45,165 M225,105 L225,105 L225,165 ">
          </path>
        </svg>
      </a>
    </div>

    <div class="nav-segments-container">
      <div class="nav-segments">
        

        <div class="flex flex-col text-left nav-links">
          <a href="/llama" class="activated-link">LLaMA</a>
          
          
          <a href="/chatgpt">ChatGPT</a>
          
          
          
          
          
          
          <a href="/tricking-llms">Tricking LLMs</a>
          
          
        </div>
        

        

        <div class="flex flex-col text-left nav-links">
          <a href="/alpaca" class="activated-link">Alpaca</a>
          
          
          
          
          
          <a href="/llama-family-tree">LLaMA Family Tree</a>
          
          
          
          <a href="/llama-timeline">LLaMA Timeline</a>
          
          
        </div>
        

        
      </div>
    </div>


  </nav>
  <main>
    <header>
      <h1>Alpaca</h1>
    </header>
    <article><p>Alpaca is a large language model fine-tuned from the <a href="/llama">LLaMA</a> 7B model on 52K instruction-following demonstrations, designed to emulate ChatGPT style instruction tuning.  It was created by researchers at Stanford.</p>
<p>The 52K instruction were generated using 175 instruction prompts entered into text-davinci-003, costing $600 in OpenAI API fees and $100 in compute for supervised training.  The initial announcement did not release their model weights as they were waiting on Meta to advise them on when and how they should do it, as at the the time the original LLaMA models were only available through leaked torrents.</p>
<p>They did release their training set and process so the model was recreated by other developers.  <a href="https://github.com/antimatter15/alpaca.cpp">alpaca.cpp</a>, a recreation of the Stanford Alpaca model, was created using <a href="/low-rank-adaptation">Low-Rank Adaptation</a> (LoRA) instruct tuning and built on top of llama.cpp.  This was then supported within <a href="https://github.com/cocktailpeanut/dalai">Dalai</a>.</p>
<p><a href="https://huggingface.co/chavinlo/alpaca-native">alpaca-native</a> is a recreation of Standford Alpaca, based on LLaMA 7B and the Stanford instruction training set using native fine-tuning (not LoRA).  This methodology was used by others to create similar models based the larger LLaMA models (see <a href="/llama-family-tree">LLaMA Family Tree</a>).</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford Alpaca announcment</a></li>
<li>Simon Willison's <a href="https://simonwillison.net/2023/Mar/13/alpaca/">Stanford Alpaca, and the acceleration of on-device large language model development</a></li>
</ul>
<h2 id="magnets">Magnets</h2>
<ul>
<li>alpaca.cpp <ul>
<li><a href="magnet:?xt=urn:btih:5aaceaec63b03e51a98f04fd5c42320b2a033010&amp;dn=ggml-alpaca-7b-q4.bin&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Fopentracker.i2p.rocks%3A6969%2Fannounce">7B model weights ggml-alpaca-7b-q4.bin</a>, 4.21GB</li>
<li><a href="magnet:?xt=urn:btih:f3cf71b172129d6b5abccab393bc32253fac8159&amp;dn=ggml-alpaca-13b-q4.bin&amp;tr=udp%3A%2F%http://2Ftracker.opentrackr.org%3A1337%2Fannounce&amp;tr=udp%3A%2F%https://t.co/zenhelfwRd%3A6969%2Fannounce&amp;tr=https%3A%2F%https://t.co/zenhelfwRd%3A443%2Fannounce&amp;tr=udp%3A%2F%https://t.co/RRAn1X65wE%3A6969%2Fannounce&amp;tr=udp%3A%2F%https://t.co/uTXBeTLUMa%3A2810%2Fannounce">13B model weights</a></li>
</ul>
</li>
</ul></article>

    <hr />

    <div class="meta">
      

      
      <p class="backlinks"><span class="bold">Incoming:</span>
        
        <a href="/llama">LLaMA</a>
        
        <a href="/llama-family-tree">LLaMA Family Tree</a>
        
        <a href="/program-names">Program Names</a>
        
        </span>
      </p>
      

      
    </div>
  </main>
  <footer>
    <a href="mailto:zachpmanson@gmail.com?subject=On 'Alpaca'">reply</a>
    <a href="/wikipedia-articles">random</a>
    <a href="/recent-changes">updates</a>
    <a href="/tags">tags</a>
    <a href="/sitemap">sitemap</a>
    <a href="https://github.dev/pavo-etc/notes/blob/main/./notes/Notes/Software/Machine Learning/Generative Models/Large Language Models/LLaMA/Alpaca.md">edit</a>
  </footer>
  
</body>

</html>